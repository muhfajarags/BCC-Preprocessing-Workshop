{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **1- About Dataset**"]},{"cell_type":"markdown","metadata":{},"source":["Survived = if passenger survived or not\n","\n","Ticket = Ticket number\n","\n","Sex = Male or Female\n","\n","Embarked = Port of Embarkation  C = Cherbourg, Q = Queenstown, S = Southampton\n","\n","Fare = Passenger fare\n","\n","Cabin = Cabin number\n","\n","pclass: Ticket class\n","\n","1st = Upper\n","\n","2nd = Middle\n","\n","3rd = Lower\n","\n","Age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n","\n","sibsp: The dataset defines family relations in this way...\n","\n","Sibling = brother, sister, stepbrother, stepsister\n","\n","Spouse = husband, wife (mistresses and fiancÃ©s were ignored)\n","\n","parch: The dataset defines family relations in this way...\n","\n","Parent = mother, father\n","\n","Child = daughter, son, stepdaughter, stepson\n","\n","Some children travelled only with a nanny, therefore parch=0 for them."]},{"cell_type":"markdown","metadata":{},"source":["# **2- importing libraries**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-04T00:34:13.662875Z","iopub.status.busy":"2024-03-04T00:34:13.662500Z","iopub.status.idle":"2024-03-04T00:34:17.021201Z","shell.execute_reply":"2024-03-04T00:34:17.020253Z","shell.execute_reply.started":"2024-03-04T00:34:13.662843Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder , StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from imblearn.over_sampling import RandomOverSampler"]},{"cell_type":"markdown","metadata":{},"source":["# **3- Loading Data**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-04T00:34:17.024739Z","iopub.status.busy":"2024-03-04T00:34:17.024049Z","iopub.status.idle":"2024-03-04T00:34:17.108675Z","shell.execute_reply":"2024-03-04T00:34:17.107302Z","shell.execute_reply.started":"2024-03-04T00:34:17.024695Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/titanic/train.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/titanic/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/titanic/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/titanic/gender_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Mufajar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Mufajar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[1;32mc:\\Users\\Mufajar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Mufajar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[1;32mc:\\Users\\Mufajar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/titanic/train.csv'"]}],"source":["train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n","test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n","submission = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\n","\n","test_df = pd.concat([test_df , submission] , axis = 1)\n","\n","train_df = train_df.drop(['PassengerId'] , axis = 1)\n","test_df = test_df.drop(['PassengerId'] , axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:56.967838Z","iopub.status.busy":"2023-09-21T08:58:56.96694Z","iopub.status.idle":"2023-09-21T08:58:56.988641Z","shell.execute_reply":"2023-09-21T08:58:56.987436Z","shell.execute_reply.started":"2023-09-21T08:58:56.967805Z"},"trusted":true},"outputs":[],"source":["train_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:56.992264Z","iopub.status.busy":"2023-09-21T08:58:56.991697Z","iopub.status.idle":"2023-09-21T08:58:57.009487Z","shell.execute_reply":"2023-09-21T08:58:57.008125Z","shell.execute_reply.started":"2023-09-21T08:58:56.992204Z"},"trusted":true},"outputs":[],"source":["test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# **4- Data Information**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:57.011543Z","iopub.status.busy":"2023-09-21T08:58:57.011132Z","iopub.status.idle":"2023-09-21T08:58:57.041369Z","shell.execute_reply":"2023-09-21T08:58:57.0401Z","shell.execute_reply.started":"2023-09-21T08:58:57.011509Z"},"trusted":true},"outputs":[],"source":["train_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:57.043161Z","iopub.status.busy":"2023-09-21T08:58:57.042776Z","iopub.status.idle":"2023-09-21T08:58:57.056499Z","shell.execute_reply":"2023-09-21T08:58:57.055306Z","shell.execute_reply.started":"2023-09-21T08:58:57.043131Z"},"trusted":true},"outputs":[],"source":["test_df.info()"]},{"cell_type":"markdown","metadata":{},"source":["### We have both missing values and categorical data and we need to handle them"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:57.058581Z","iopub.status.busy":"2023-09-21T08:58:57.058147Z","iopub.status.idle":"2023-09-21T08:58:57.072421Z","shell.execute_reply":"2023-09-21T08:58:57.071284Z","shell.execute_reply.started":"2023-09-21T08:58:57.058544Z"},"trusted":true},"outputs":[],"source":["train_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:57.073579Z","iopub.status.busy":"2023-09-21T08:58:57.073282Z","iopub.status.idle":"2023-09-21T08:58:57.088183Z","shell.execute_reply":"2023-09-21T08:58:57.087339Z","shell.execute_reply.started":"2023-09-21T08:58:57.073554Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'test_df' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_df\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n","\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"]}],"source":["test_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["### 'Cabin' has a lot of missing values (about 70%) so we have to drop it during missing value handling\n","### We can use imputers (simple imputer / iterative imputer) in order to handle Nan values for 'Age' and 'Embarked'"]},{"cell_type":"markdown","metadata":{},"source":["# **5- Data Visualization**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:57.089359Z","iopub.status.busy":"2023-09-21T08:58:57.089049Z","iopub.status.idle":"2023-09-21T08:58:59.287024Z","shell.execute_reply":"2023-09-21T08:58:59.286041Z","shell.execute_reply.started":"2023-09-21T08:58:57.089335Z"},"trusted":true},"outputs":[],"source":["visual_data = train_df.drop(['Ticket' ,'Name'  , 'Cabin'] , axis = 1)\n","\n","num_cols = len(visual_data.columns)\n","num_rows = (num_cols + 2) // 3\n","\n","fig, axes = plt.subplots(num_rows, 3, figsize=(12, 3 * num_rows))\n","axes = axes.flatten()\n","\n","for i, column in enumerate(visual_data.columns):\n","    ax = axes[i]\n","    sns.histplot(visual_data[column], ax=ax, color = \"maroon\")\n","    ax.set_title(column)\n","\n","for i in range(num_cols, num_rows * 3):\n","    fig.delaxes(axes[i])\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Most of the passengers were Male , between 15 - 35 years old , embarked for southampton and ...\n"]},{"cell_type":"markdown","metadata":{},"source":["# **6- Data Preprocessing**"]},{"cell_type":"markdown","metadata":{},"source":["# 6-1 Clean up and Removing useless features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:59.29082Z","iopub.status.busy":"2023-09-21T08:58:59.290157Z","iopub.status.idle":"2023-09-21T08:58:59.320588Z","shell.execute_reply":"2023-09-21T08:58:59.319492Z","shell.execute_reply.started":"2023-09-21T08:58:59.290787Z"},"trusted":true},"outputs":[],"source":["def preprocess(df):\n","    \n","    def normalize_name(x):\n","        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n","    \n","    def ticket_number(x):\n","        return x.split(\" \")[-1]\n","        \n","    def ticket_item(x):\n","        items = x.split(\" \")\n","        if len(items) == 1:\n","            return \"NONE\"\n","        return \"_\".join(items[0:-1])\n","    \n","    \n","    train_df[\"Ticket_number\"] = train_df[\"Ticket\"].apply(ticket_number)\n","    train_df[\"Ticket_item\"] = train_df[\"Ticket\"].apply(ticket_item)  \n","    test_df[\"Ticket_number\"] = test_df[\"Ticket\"].apply(ticket_number)\n","    test_df[\"Ticket_item\"] = test_df[\"Ticket\"].apply(ticket_item) \n","\n","    return df\n","    \n","train_df = preprocess(train_df)\n","test_df = preprocess(test_df)\n","\n","train_df = train_df.drop(['Name' ,'Ticket'] , axis = 1) # Ticket replaced with Ticket_number and Ticket item\n","test_df = test_df.drop(['Name' ,'Ticket'] , axis = 1) # Ticket replaced with Ticket_number and Ticket item\n","\n","train_df.head(5)\n","\n","\n","# Copied from https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests?scriptVersionId=130042040&cellId=7"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:59.322803Z","iopub.status.busy":"2023-09-21T08:58:59.322138Z","iopub.status.idle":"2023-09-21T08:58:59.343833Z","shell.execute_reply":"2023-09-21T08:58:59.342958Z","shell.execute_reply.started":"2023-09-21T08:58:59.322763Z"},"trusted":true},"outputs":[],"source":["test_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### **Note :**\n","The order in which you handle categorical values and missing values in your data preprocessing pipeline can depend on the specific characteristics of your dataset and the machine learning algorithm you plan to use. However, there are some general guidelines that can help you decide the order:\n","\n","**Handle Missing Values First:** In most cases, it's a good practice to address missing values before encoding categorical variables. Missing data can have a significant impact on the performance of your machine learning models, and addressing them early can help ensure that your models are not biased or inaccurate due to missing information. You can use techniques like imputation (e.g., filling missing values with the mean, median, or mode) or advanced methods like predictive imputation or interpolation.\n","\n","**Encode Categorical Variables:** Once you've dealt with missing values, you can proceed with encoding categorical variables. This step involves converting categorical data into a numerical format that machine learning algorithms can work with. Common encoding techniques include one-hot encoding, label encoding, or ordinal encoding, depending on the nature of your categorical data.\n","\n","**Consider Simultaneous Handling:** In some cases, you might need to handle missing values and encode categorical variables simultaneously. For example, you might want to impute missing values with a value specific to the category or use different imputation techniques for different categorical columns. This requires careful consideration of the relationships between missing values and categorical variables in your dataset.\n"]},{"cell_type":"markdown","metadata":{},"source":["# 6-2 Missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:59.346141Z","iopub.status.busy":"2023-09-21T08:58:59.345185Z","iopub.status.idle":"2023-09-21T08:58:59.746056Z","shell.execute_reply":"2023-09-21T08:58:59.74499Z","shell.execute_reply.started":"2023-09-21T08:58:59.346101Z"},"trusted":true},"outputs":[],"source":["null_count_columns = train_df.isnull().sum()\n","\n","null_counts_df = pd.DataFrame(list(null_count_columns.items()), columns=[\"Column\", \"NullCount\"])\n","\n","null_counts_df.sort_values(by=\"NullCount\", inplace=True)\n","\n","plt.figure(figsize=(10, 6))\n","plt.barh(null_counts_df[\"Column\"], null_counts_df[\"NullCount\"], color=\"maroon\")\n","plt.xlabel(\"Null Counts\")\n","plt.ylabel(\"Columns\")\n","plt.title(\"Null Counts in Dataset Columns\")\n","plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:58:59.747944Z","iopub.status.busy":"2023-09-21T08:58:59.747588Z","iopub.status.idle":"2023-09-21T08:59:00.142713Z","shell.execute_reply":"2023-09-21T08:59:00.141432Z","shell.execute_reply.started":"2023-09-21T08:58:59.747915Z"},"trusted":true},"outputs":[],"source":["null_count_columns = test_df.isnull().sum()\n","\n","null_counts_df = pd.DataFrame(list(null_count_columns.items()), columns=[\"Column\", \"NullCount\"])\n","\n","null_counts_df.sort_values(by=\"NullCount\", inplace=True)\n","\n","plt.figure(figsize=(10, 6))\n","plt.barh(null_counts_df[\"Column\"], null_counts_df[\"NullCount\"], color=\"maroon\")\n","plt.xlabel(\"Null Counts\")\n","plt.ylabel(\"Columns\")\n","plt.title(\"Null Counts in Dataset Columns\")\n","plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n","plt.tight_layout()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:59:00.145284Z","iopub.status.busy":"2023-09-21T08:59:00.14461Z","iopub.status.idle":"2023-09-21T08:59:00.198768Z","shell.execute_reply":"2023-09-21T08:59:00.197559Z","shell.execute_reply.started":"2023-09-21T08:59:00.145241Z"},"trusted":true},"outputs":[],"source":["# At first 'Cabin' column should drop from dataset because of 70% missing values\n","train_df = train_df.drop(['Cabin'] , axis = 1)\n","test_df = test_df.drop(['Cabin'] , axis = 1)\n","\n","# I want to use simple imputers to fill nan values for Age and Fare and Embarked\n","imp = SimpleImputer(strategy = 'mean')\n","train_df[['Age', 'Fare']] = imp.fit_transform(train_df[['Age', 'Fare']])\n","train_df[['Age', 'Fare']] = pd.DataFrame(train_df[['Age', 'Fare']] , columns = ['Age' , 'Fare'])\n","\n","test_df[['Age', 'Fare']] = imp.fit_transform(test_df[['Age', 'Fare']])\n","test_df[['Age', 'Fare']] = pd.DataFrame(test_df[['Age', 'Fare']] , columns = ['Age' , 'Fare'])\n","\n","\n","imp = SimpleImputer(strategy = 'most_frequent')\n","train_df[['Embarked']] = imp.fit_transform(train_df[['Embarked']])\n","train_df[['Embarked']] = pd.DataFrame(train_df[['Embarked']] , columns = ['Embarked'])\n","\n","test_df[['Embarked']] = imp.fit_transform(test_df[['Embarked']])\n","test_df[['Embarked']] = pd.DataFrame(test_df[['Embarked']] , columns = ['Embarked'])\n","\n","train_df.head(7)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:59:00.203926Z","iopub.status.busy":"2023-09-21T08:59:00.2002Z","iopub.status.idle":"2023-09-21T08:59:00.220451Z","shell.execute_reply":"2023-09-21T08:59:00.219143Z","shell.execute_reply.started":"2023-09-21T08:59:00.203887Z"},"trusted":true},"outputs":[],"source":["test_df.head(7)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:59:00.222554Z","iopub.status.busy":"2023-09-21T08:59:00.222179Z","iopub.status.idle":"2023-09-21T08:59:00.237608Z","shell.execute_reply":"2023-09-21T08:59:00.236287Z","shell.execute_reply.started":"2023-09-21T08:59:00.222515Z"},"trusted":true},"outputs":[],"source":["train_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:59:00.239726Z","iopub.status.busy":"2023-09-21T08:59:00.239212Z","iopub.status.idle":"2023-09-21T08:59:00.268118Z","shell.execute_reply":"2023-09-21T08:59:00.26721Z","shell.execute_reply.started":"2023-09-21T08:59:00.239687Z"},"trusted":true},"outputs":[],"source":["test_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["### HOORAY ! There is no missing value left"]},{"cell_type":"markdown","metadata":{},"source":["# 6-3 Categorical data"]},{"cell_type":"markdown","metadata":{},"source":["When dealing with structured, tabular data (which we usually be doing), the first question we generally ask ourselves is whether the values are of a numeric or categorical nature. Types of Data:\n","\n","Qualitative Data\n","\n","Quantitative Data\n","\n","Qualitative Data: âData Associated with the quality in different categoriesâ. Data is measurements, each fall into one of several categories. (Hair Color, ethnic groups and other attributes of the population)\n","\n","(a). Nominal Data: âWith no inherent order or rankingâ ~ Data with no inherent order or ranking such as gender or race, such kind of data called Nominal Data.\n","\n","(b). Ordinal Data: âwith an order seriesâ\n","\n","Quantitative Data: âData associated with Quantity which can be measuredâ ~ Data measured on a numeric scale (distance travelled to college, the number of children in a family etc.)\n","(a). Discrete Data: âBased on count, finite number of values possible and value cannot be subdividedâ ~ Data which can be categorized into classification, data which is based upon counts, there are only a finite number of values possible and values cannot be subdivided meaningfully, such kind of data is called Discrete Data.\n","\n","(b). Continuous Data: âmeasured on a continuum or a scale, value which can be subdivided into finer incrementsâ ~ Data which can be measured on a continuum or a scale, data which can be have almost any numeric value and can be subdivided into finer and finer increments, such kind of data is called Continuous Data.\n","\n","Usually there are 2 kinds of categorical data:\n","\n","â Ordinal Data: The categories have an inherent order like: socio economic status (âlow incomeâ,âmiddle incomeâ,âhigh incomeâ), education level (âhigh schoolâ,âBSâ,âMSâ,âPhDâ), income level (âless than 50Kâ, â50K-100Kâ, âover 100Kâ), satisfaction rating (âextremely dislikeâ, âdislikeâ, âneutralâ, âlikeâ, âextremely likeâ)\n","\n","â Nominal Data: The categories do not have an inherent order like: blood type, zip code, gender, race, ethnicity Also binary data would be nominal or ordinal. Generally, In Ordinal data, while encoding, one should retain the information regarding the order in which the category is provided. While encoding Nominal data, we have to consider the presence or absence of a feature. In such a case, no notion of order is present.\n","\n","So how should we select encoding methods is depends algorithm(s) we apply :\n","\n","â Some algorithms can work with categorical data directly or For example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation). So Some implementations of machine learning algorithms require all data to be numerical. For example, scikit-learn has this requirement.\n","\n","â If we categorize algorithms to linear and tree based models we should consider that generally linear models are sensitive to order of ordinal data so we should select appropriate encoding methods."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:59:00.270454Z","iopub.status.busy":"2023-09-21T08:59:00.26979Z","iopub.status.idle":"2023-09-21T08:59:00.298175Z","shell.execute_reply":"2023-09-21T08:59:00.296925Z","shell.execute_reply.started":"2023-09-21T08:59:00.270414Z"},"trusted":true},"outputs":[],"source":["# we have lots of categorical features so before using iterative imputer its better to encode nominal and ordinal categorical data\n","nominals = ['Sex' , 'Embarked' ,\"Ticket_item\"]\n","ordinals = ['Pclass' , \"Parch\" , \"SibSp\"] # values are numeric and meaningful so there is no need to encode\n","\n","# we want to use LabelEncoding for nominal values\n","le = LabelEncoder()\n","train_df['Sex'] = le.fit_transform(train_df['Sex'])\n","train_df['Embarked'] = le.fit_transform(train_df['Embarked'])\n","train_df['Ticket_item'] = le.fit_transform(train_df['Ticket_item'])\n","\n","test_df['Sex'] = le.fit_transform(test_df['Sex'])\n","test_df['Embarked'] = le.fit_transform(test_df['Embarked'])\n","test_df['Ticket_item'] = le.fit_transform(test_df['Ticket_item'])\n","\n","#Ticket_number column is defined as an object type so we need to convert its type to int or float\n","train_df['Ticket_number'] = pd.to_numeric(train_df['Ticket_number'], errors='coerce')\n","test_df['Ticket_number'] = pd.to_numeric(test_df['Ticket_number'], errors='coerce')\n","\n","train_df.head(7)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:59:10.709683Z","iopub.status.busy":"2023-09-21T08:59:10.708943Z","iopub.status.idle":"2023-09-21T08:59:10.722Z","shell.execute_reply":"2023-09-21T08:59:10.721205Z","shell.execute_reply.started":"2023-09-21T08:59:10.709645Z"},"trusted":true},"outputs":[],"source":["train_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T08:59:20.472315Z","iopub.status.busy":"2023-09-21T08:59:20.471615Z","iopub.status.idle":"2023-09-21T08:59:20.486465Z","shell.execute_reply":"2023-09-21T08:59:20.485307Z","shell.execute_reply.started":"2023-09-21T08:59:20.472268Z"},"trusted":true},"outputs":[],"source":["test_df.info()"]},{"cell_type":"markdown","metadata":{},"source":["### There is no categorical value left now we can go for outliers"]},{"cell_type":"markdown","metadata":{},"source":["# 6-4 Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:00:18.621741Z","iopub.status.busy":"2023-09-21T09:00:18.621326Z","iopub.status.idle":"2023-09-21T09:00:18.66233Z","shell.execute_reply":"2023-09-21T09:00:18.661071Z","shell.execute_reply.started":"2023-09-21T09:00:18.62171Z"},"trusted":true},"outputs":[],"source":["train_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:00:30.881848Z","iopub.status.busy":"2023-09-21T09:00:30.881456Z","iopub.status.idle":"2023-09-21T09:00:30.920634Z","shell.execute_reply":"2023-09-21T09:00:30.919511Z","shell.execute_reply.started":"2023-09-21T09:00:30.881817Z"},"trusted":true},"outputs":[],"source":["test_df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["#### I want to remove Outliers with Z_SCORE method and i want to choose [ -3 , 3 ] as my threshold ....\n","\n","A common threshold is to consider data points with z-scores outside of the range [-3, 3] as outliers. This means that any data point with a z-score less than -3 or greater than 3 would be considered an outlier. Here's a brief explanation of why this range is often used:\n","\n","**Standard Deviation Interpretation:** In a standard normal distribution (mean = 0, standard deviation = 1), about 99.7% of the data falls within 3 standard deviations of the mean. Therefore, by setting the threshold at 3 standard deviations (z-scores), you are effectively capturing the majority of the data points and treating those outside this range as outliers.\n","\n","**Adjustable Sensitivity:** You can adjust the threshold based on your specific requirements. If you want to be more conservative and capture even fewer outliers, you could use a smaller range, like [-2, 2]. Conversely, if you want to be more permissive and capture more potential outliers, you could use a larger range, like [-4, 4].\n","\n","**Data Distribution:** The choice of threshold may also depend on the distribution of your data. For data that doesn't follow a normal distribution, a z-score threshold may not be the best approach. In such cases, you might consider using other methods, such as the Interquartile Range (IQR) or domain-specific knowledge.\n","\n","Remember that the choice of threshold is somewhat arbitrary, and it's important to consider the context of your analysis, the characteristics of your data, and your specific goals when deciding on an appropriate threshold for identifying outliers. Additionally, it's often a good practice to visualize your data before and after removing outliers to assess the impact of your choices."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:00:53.111408Z","iopub.status.busy":"2023-09-21T09:00:53.110339Z","iopub.status.idle":"2023-09-21T09:00:53.13526Z","shell.execute_reply":"2023-09-21T09:00:53.134192Z","shell.execute_reply.started":"2023-09-21T09:00:53.111371Z"},"trusted":true},"outputs":[],"source":["threshold = 3\n","# identify outliers using z-score method\n","\n","# Calculate the z-scores for each data point\n","z_scores = np.abs((train_df - train_df.mean()) / train_df.std())\n","\n","# outliers by filtering based on the z-scores\n","outliers = z_scores > threshold\n","\n","# Now 'no_outliers' contains the data without outliers\n","outliers.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:01:02.543801Z","iopub.status.busy":"2023-09-21T09:01:02.543108Z","iopub.status.idle":"2023-09-21T09:01:02.566632Z","shell.execute_reply":"2023-09-21T09:01:02.565387Z","shell.execute_reply.started":"2023-09-21T09:01:02.543763Z"},"trusted":true},"outputs":[],"source":["threshold = 3\n","# identify outliers using z-score method\n","\n","# Calculate the z-scores for each data point\n","z_scores = np.abs((test_df - test_df.mean()) / test_df.std())\n","\n","# outliers by filtering based on the z-scores\n","outliers = z_scores > threshold\n","\n","# Now 'no_outliers' contains the data without outliers\n","outliers.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:01:24.634611Z","iopub.status.busy":"2023-09-21T09:01:24.633575Z","iopub.status.idle":"2023-09-21T09:01:24.676954Z","shell.execute_reply":"2023-09-21T09:01:24.676027Z","shell.execute_reply.started":"2023-09-21T09:01:24.634565Z"},"trusted":true},"outputs":[],"source":["# removing outliers using z-score method\n","\n","# Calculate the z-scores for each train point\n","z_scores = np.abs((train_df - train_df.mean()) / train_df.std())\n","\n","# Remove outliers by filtering based on the z-scores\n","train_df = train_df[(z_scores <= threshold)]\n","\n","\n","# Calculate the z-scores for each test point\n","z_scores = np.abs((test_df - test_df.mean()) / test_df.std())\n","\n","# Remove outliers by filtering based on the z-scores\n","test_df = test_df[(z_scores <= threshold)]\n","\n","\n","train_df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:01:32.888634Z","iopub.status.busy":"2023-09-21T09:01:32.88765Z","iopub.status.idle":"2023-09-21T09:01:32.904444Z","shell.execute_reply":"2023-09-21T09:01:32.903556Z","shell.execute_reply.started":"2023-09-21T09:01:32.888588Z"},"trusted":true},"outputs":[],"source":["test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:01:42.592777Z","iopub.status.busy":"2023-09-21T09:01:42.592415Z","iopub.status.idle":"2023-09-21T09:01:42.603394Z","shell.execute_reply":"2023-09-21T09:01:42.602141Z","shell.execute_reply.started":"2023-09-21T09:01:42.59275Z"},"trusted":true},"outputs":[],"source":["train_df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:01:52.023437Z","iopub.status.busy":"2023-09-21T09:01:52.023048Z","iopub.status.idle":"2023-09-21T09:01:52.033395Z","shell.execute_reply":"2023-09-21T09:01:52.032266Z","shell.execute_reply.started":"2023-09-21T09:01:52.023408Z"},"trusted":true},"outputs":[],"source":["test_df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["### OH !!!!! again missing values\n","### This NaN values are because of removing outliers and we need to handle them again"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:02:16.772288Z","iopub.status.busy":"2023-09-21T09:02:16.771869Z","iopub.status.idle":"2023-09-21T09:02:16.791859Z","shell.execute_reply":"2023-09-21T09:02:16.790725Z","shell.execute_reply.started":"2023-09-21T09:02:16.772255Z"},"trusted":true},"outputs":[],"source":["# I want to drop NaN values because they are not too much\n","train_df = train_df.dropna()\n","test_df = test_df.dropna()\n","train_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:02:27.831557Z","iopub.status.busy":"2023-09-21T09:02:27.831141Z","iopub.status.idle":"2023-09-21T09:02:27.845372Z","shell.execute_reply":"2023-09-21T09:02:27.844134Z","shell.execute_reply.started":"2023-09-21T09:02:27.831521Z"},"trusted":true},"outputs":[],"source":["test_df.info()"]},{"cell_type":"markdown","metadata":{},"source":["# 6-5 Splitting to test , validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:02:49.120643Z","iopub.status.busy":"2023-09-21T09:02:49.12016Z","iopub.status.idle":"2023-09-21T09:02:49.133413Z","shell.execute_reply":"2023-09-21T09:02:49.132538Z","shell.execute_reply.started":"2023-09-21T09:02:49.120611Z"},"trusted":true},"outputs":[],"source":["# Train set is ok to be trained on our models but we need validation so we are going to split test set to test and validation set\n","X_train = train_df.drop('Survived' , axis =1)\n","y_train = train_df['Survived']\n","\n","test_df_X = test_df.drop('Survived' , axis =1)\n","test_df_y = test_df['Survived']\n","\n","X_test ,X_val ,y_test ,y_val = train_test_split( test_df_X , test_df_y , test_size = 0.5 ,random_state =42)\n","\n","print(\"X_train shape:\", X_train.shape)\n","print(\"y_train shape:\", y_train.shape,\"\\n\")\n","\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_test shape:\", y_test.shape,\"\\n\")\n","\n","print(\"X_test validation:\", X_val.shape)\n","print(\"y_test validation:\", y_val.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# 6-6 Data transformation"]},{"cell_type":"markdown","metadata":{},"source":["#### **Standardize features by removing the mean and scaling to unit variance.**\n","\n","#### The standard score of a sample x is calculated as:\n","\n","#### z = (x - u) / s\n","\n","#### where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:03:40.05548Z","iopub.status.busy":"2023-09-21T09:03:40.05507Z","iopub.status.idle":"2023-09-21T09:03:40.071661Z","shell.execute_reply":"2023-09-21T09:03:40.070683Z","shell.execute_reply.started":"2023-09-21T09:03:40.055452Z"},"trusted":true},"outputs":[],"source":["scaler = StandardScaler()\n","X_train = pd.DataFrame(scaler.fit_transform(X_train) , columns = X_train.columns)\n","X_val = pd.DataFrame(scaler.transform(X_val) , columns = X_val.columns)\n","X_test = pd.DataFrame(scaler.transform(X_test) , columns = X_test.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:03:49.425961Z","iopub.status.busy":"2023-09-21T09:03:49.424976Z","iopub.status.idle":"2023-09-21T09:03:49.442293Z","shell.execute_reply":"2023-09-21T09:03:49.440928Z","shell.execute_reply.started":"2023-09-21T09:03:49.425924Z"},"trusted":true},"outputs":[],"source":["X_train.head(7)"]},{"cell_type":"markdown","metadata":{},"source":["# 6-7 Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:04:17.592851Z","iopub.status.busy":"2023-09-21T09:04:17.592007Z","iopub.status.idle":"2023-09-21T09:04:18.032682Z","shell.execute_reply":"2023-09-21T09:04:18.031337Z","shell.execute_reply.started":"2023-09-21T09:04:17.592814Z"},"trusted":true},"outputs":[],"source":["corrmat = X_train.corr()\n","f, ax = plt.subplots(figsize=(10, 10))\n","sns.heatmap(corrmat, vmax= 1, square=True);"]},{"cell_type":"markdown","metadata":{},"source":["# 6-7 Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:04:40.736568Z","iopub.status.busy":"2023-09-21T09:04:40.736165Z","iopub.status.idle":"2023-09-21T09:04:41.238406Z","shell.execute_reply":"2023-09-21T09:04:41.237306Z","shell.execute_reply.started":"2023-09-21T09:04:40.736538Z"},"trusted":true},"outputs":[],"source":["model = RandomForestClassifier(n_estimators=100, random_state=42)\n","model.fit(X_train, y_train)\n","\n","feature_importances = model.feature_importances_\n","\n","importances_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n","importances_df = importances_df.sort_values(by='Importance', ascending=False)\n","\n","plt.figure(figsize=(10, 6) ,)\n","plt.barh(importances_df['Feature'], importances_df['Importance'] , color = 'Maroon')\n","plt.xlabel('Importance')\n","plt.ylabel('Feature')\n","plt.title('Feature Importance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### All features are important and there is no need to drop any of them"]},{"cell_type":"markdown","metadata":{},"source":["# 6-9 Imbalanced Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:05:20.064994Z","iopub.status.busy":"2023-09-21T09:05:20.064614Z","iopub.status.idle":"2023-09-21T09:05:20.073638Z","shell.execute_reply":"2023-09-21T09:05:20.072538Z","shell.execute_reply.started":"2023-09-21T09:05:20.064964Z"},"trusted":true},"outputs":[],"source":["y_train.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:05:32.652726Z","iopub.status.busy":"2023-09-21T09:05:32.649708Z","iopub.status.idle":"2023-09-21T09:05:32.836675Z","shell.execute_reply":"2023-09-21T09:05:32.835024Z","shell.execute_reply.started":"2023-09-21T09:05:32.652685Z"},"trusted":true},"outputs":[],"source":["f,ax=plt.subplots(1,1,figsize=(12,8))\n","y_train.value_counts().plot.pie(explode=[0,0.01],autopct='%1.1f%%',shadow=False , colors = ['#850428','#328f60'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:05:42.268595Z","iopub.status.busy":"2023-09-21T09:05:42.268203Z","iopub.status.idle":"2023-09-21T09:05:42.285242Z","shell.execute_reply":"2023-09-21T09:05:42.283943Z","shell.execute_reply.started":"2023-09-21T09:05:42.268567Z"},"trusted":true},"outputs":[],"source":["oversampler = RandomOverSampler(random_state=42)\n","X_train, y_train = oversampler.fit_resample(X_train, y_train)\n","y_train.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-21T09:05:49.569482Z","iopub.status.busy":"2023-09-21T09:05:49.568732Z","iopub.status.idle":"2023-09-21T09:05:49.716301Z","shell.execute_reply":"2023-09-21T09:05:49.714798Z","shell.execute_reply.started":"2023-09-21T09:05:49.569448Z"},"trusted":true},"outputs":[],"source":["f,ax=plt.subplots(1,1,figsize=(12,8))\n","y_train.value_counts().plot.pie(explode=[0,0.01],autopct='%1.1f%%',shadow=False , colors = ['#850428','#328f60'])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Finally we did Preprocessing on our dataset. Now its the time to Model our data with Neural Network"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":26502,"sourceId":3136,"sourceType":"competition"}],"dockerImageVersionId":30558,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
